{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries \n",
    "\n",
    "`nltk` is a natural language processing toolkit: http://www.nltk.org/\n",
    "In this example, we will use NLTK for part-of-speech tagging and for lemmatization.\n",
    "\n",
    "`re` is a library for regular expressions.\n",
    "\n",
    "`math`, `scipy`, and `numpy` have useful math, statistics, and linear algebra functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "1. Read in text.\n",
    "2. Extract sentences and tokens.\n",
    "3. Lowercase all tokens.\n",
    "4. Tag each token with its POS tag.\n",
    "5. Lemmatize each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All flyers were given a purpose and the crow was left without one. He went to visit a bear and the bear spent weeks teaching him about medicine. The crow was still mad without having a purpose. One day he heard a squirrel crying in a hollow oak tree. He went to go see and saw the squirrel was sad about something.  He directed the squirrel to the bear, who was good with medicine. The next the day crow flew around satisfied with knowing that he had helped someone. He heard bunny crying in their burrow. He asked asked why and they said that they could never they could never get any peace with the fox always prying around. Then the crow stated the bunny's strengths of big ears and strong legs. Then word came around of crow's purpose of helping others find their purpose.\n"
     ]
    }
   ],
   "source": [
    "# Read in our text file\n",
    "filename = 'data/crow_retelling.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All flyers were given a purpose and the crow was left without one', 'He went to visit a bear and the bear spent weeks teaching him about medicine', 'The crow was still mad without having a purpose', 'One day he heard a squirrel crying in a hollow oak tree', 'He went to go see and saw the squirrel was sad about something', ' He directed the squirrel to the bear, who was good with medicine', 'The next the day crow flew around satisfied with knowing that he had helped someone', 'He heard bunny crying in their burrow', 'He asked asked why and they said that they could never they could never get any peace with the fox always prying around', \"Then the crow stated the bunny's strengths of big ears and strong legs\", \"Then word came around of crow's purpose of helping others find their purpose.\"]\n"
     ]
    }
   ],
   "source": [
    "# Split the text into sentences\n",
    "sentences = text.split('. ')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can tokenize our text (i.e., split into tokens) and lowercase. We use NLTK's word tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'flyers', 'were', 'given', 'a', 'purpose', 'and', 'the', 'crow', 'was', 'left', 'without', 'one', '.', 'he', 'went', 'to', 'visit', 'a', 'bear', 'and', 'the', 'bear', 'spent', 'weeks', 'teaching', 'him', 'about', 'medicine', '.', 'the', 'crow', 'was', 'still', 'mad', 'without', 'having', 'a', 'purpose', '.', 'one', 'day', 'he', 'heard', 'a', 'squirrel', 'crying', 'in', 'a', 'hollow', 'oak', 'tree', '.', 'he', 'went', 'to', 'go', 'see', 'and', 'saw', 'the', 'squirrel', 'was', 'sad', 'about', 'something', '.', 'he', 'directed', 'the', 'squirrel', 'to', 'the', 'bear', ',', 'who', 'was', 'good', 'with', 'medicine', '.', 'the', 'next', 'the', 'day', 'crow', 'flew', 'around', 'satisfied', 'with', 'knowing', 'that', 'he', 'had', 'helped', 'someone', '.', 'he', 'heard', 'bunny', 'crying', 'in', 'their', 'burrow', '.', 'he', 'asked', 'asked', 'why', 'and', 'they', 'said', 'that', 'they', 'could', 'never', 'they', 'could', 'never', 'get', 'any', 'peace', 'with', 'the', 'fox', 'always', 'prying', 'around', '.', 'then', 'the', 'crow', 'stated', 'the', 'bunny', \"'s\", 'strengths', 'of', 'big', 'ears', 'and', 'strong', 'legs', '.', 'then', 'word', 'came', 'around', 'of', 'crow', \"'s\", 'purpose', 'of', 'helping', 'others', 'find', 'their', 'purpose', '.']\n"
     ]
    }
   ],
   "source": [
    "# Split our text into lowercased tokens\n",
    "tokens = []\n",
    "for token in nltk.word_tokenize(text):\n",
    "    tokens += [token.lower()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS tagging**: Assign parts of speech to each token, such as noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 (token, tag) pairs:\n",
      "[('all', 'DT'), ('flyers', 'NNS'), ('were', 'VBD'), ('given', 'VBN'), ('a', 'DT'), ('purpose', 'NN'), ('and', 'CC'), ('the', 'DT'), ('crow', 'NN'), ('was', 'VBD'), ('left', 'VBN'), ('without', 'IN'), ('one', 'CD'), ('.', '.'), ('he', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "# Tag our tokens with the Stanford POS tagger: https://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "pos_tagger_model = \"libraries/stanford-postagger-2012-01-06/models/english-left3words-distsim.tagger\"\n",
    "pos_tagger_jar_file = \"libraries/stanford-postagger-2012-01-06/stanford-postagger.jar\"\n",
    "pos_tagger = nltk.tag.StanfordPOSTagger(pos_tagger_model, path_to_jar=pos_tagger_jar_file)\n",
    "\n",
    "pos_tagged_tokens = pos_tagger.tag(tokens)\n",
    "print(\"First 15 (token, tag) pairs:\")\n",
    "print(pos_tagged_tokens[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to filter out the punctuation tags. \n",
    "We can rely on the POS to determine whether or not a token is a punctuation. We remove any (word, tag) pair in which the tag it not made up of characters.\n",
    "\n",
    "**Lemmatization**: Normalizing a token by removing inflectional endings and returning the base or dictionary form of a word, which is known as the lemma.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the WordNet lemmatizer\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function used to convert the POS tags from the Stanford parser to the WordNet format\n",
    "def pos_stanford2wordnet(treebank_tag):\n",
    "    '''Map the Stanford POS tags to WordNet tags.'''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        # default\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tokens:\n",
      "[('all', 'DT'), ('flyers', 'NNS'), ('were', 'VBD'), ('given', 'VBN'), ('a', 'DT'), ('purpose', 'NN'), ('and', 'CC'), ('the', 'DT'), ('crow', 'NN'), ('was', 'VBD'), ('left', 'VBN'), ('without', 'IN'), ('one', 'CD'), ('he', 'PRP'), ('went', 'VBD')]\n",
      "\n",
      "lemmatized_tokens:\n",
      "['all', 'flyer', 'be', 'give', 'a', 'purpose', 'and', 'the', 'crow', 'be', 'leave', 'without', 'one', 'he', 'go']\n"
     ]
    }
   ],
   "source": [
    "# Filter out any punctuation tags\n",
    "regex_pos_content = re.compile(r'^[a-zA-Z$]+$')\n",
    "pos_tokens = [] \n",
    "lemmatized_tokens = [] \n",
    "for token, tag in pos_tagged_tokens:\n",
    "    \n",
    "    if regex_pos_content.findall(tag):\n",
    "        pos_tokens += [(token, tag)]\n",
    "        lemmatized_tokens += [lemmatizer.lemmatize(token, pos_stanford2wordnet(tag))]\n",
    "\n",
    "print('pos_tokens:')\n",
    "print(pos_tokens[:15])\n",
    "print('\\nlemmatized_tokens:')\n",
    "print(lemmatized_tokens[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we've done so far:\n",
      "Sentence:  All flyers were given a purpose and the crow was left without one\n",
      "Lowercased tokens:  ['all', 'flyers', 'were', 'given', 'a', 'purpose', 'and', 'the', 'crow', 'was', 'left', 'without', 'one']\n",
      "POS tagged tokens:  [('all', 'DT'), ('flyers', 'NNS'), ('were', 'VBD'), ('given', 'VBN'), ('a', 'DT'), ('purpose', 'NN'), ('and', 'CC'), ('the', 'DT'), ('crow', 'NN'), ('was', 'VBD'), ('left', 'VBN'), ('without', 'IN'), ('one', 'CD')]\n",
      "Lemmatized tokens:  ['all', 'flyer', 'be', 'give', 'a', 'purpose', 'and', 'the', 'crow', 'be', 'leave', 'without', 'one']\n"
     ]
    }
   ],
   "source": [
    "print(\"What we've done so far:\")\n",
    "print('Sentence: ', sentences[0])\n",
    "print('Lowercased tokens: ', tokens[:13])\n",
    "print('POS tagged tokens: ', pos_tokens[:13])\n",
    "print('Lemmatized tokens: ', lemmatized_tokens[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done with pre-processing!\n",
    "\n",
    "Now, we can go ahead and extract linguistic features!\n",
    "\n",
    "# Linguistic feature extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_linguistic_features(feat_dict):\n",
    "    print(\"Linguistic features\")\n",
    "    for feature in feat_dict:\n",
    "        print(feature, ': ', feat_dict[feature])\n",
    "linguistic_features = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by retrieving the total number of sentences and of tokens, and the average length of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences:  11\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(sentences)\n",
    "print(\"# of sentences: \", num_sentences)\n",
    "linguistic_features['num_sentences'] = num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens:  147\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(lemmatized_tokens) # Recall: lemmatized_tokens has all punctuation removed\n",
    "print(\"# of tokens: \", num_tokens)\n",
    "linguistic_features['num_tokens'] = num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length:  4.210884353741497\n"
     ]
    }
   ],
   "source": [
    "# Word length\n",
    "word_length_sum = 0\n",
    "for pos_token in pos_tokens:\n",
    "    token = pos_token[0]\n",
    "    word_length_sum += len(token)\n",
    "average_word_length = word_length_sum / num_tokens\n",
    "print('Average word length: ', average_word_length )\n",
    "linguistic_features['average_word_length'] = average_word_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary richness\n",
    "\n",
    "We'll look at a few metrics for measuring vocabulary richness.\n",
    "\n",
    "- The **type-token-ratio (TTR)** computes the ratio of the total number of types (i.e., unique words) over the total number of tokens (i.e., all words). The **moving average type-token-ratio (MATTR)** computes the TTR over a window of size k, and averages over the total number of windows\n",
    "\n",
    "- The **Brunet index** computes vocabulary richness according to the following equation: \\begin{equation}N^{(U^{-0.165})}\\end{equation} where *N* is total number of words and *U* is the total number of unique words.\n",
    "\n",
    "- The **Honoré statistic** computes vocabulary richness according to the following equation:\n",
    "\\begin{equation}\\frac{100 \\log N}{1 - \\frac{N_1}{U}}\\end{equation} \n",
    "where *N* is the total number of words, *U* is the total number of unique words, and *N1* is the number of *hapax legomenon* (i.e., words only used once)\n",
    "\n",
    "\n",
    "\n",
    "Let's start with computing TTR and MATTR. We first compute the total number of **types** and the total number of **tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # word types:  84\n",
      "Total # work tokens:  147\n"
     ]
    }
   ],
   "source": [
    "num_word_types = len(set(pos_tokens)) \n",
    "print('Total # word types: ', num_word_types)\n",
    "print('Total # work tokens: ', num_tokens) # We already computed num_tokens before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTR:  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "ttr = num_word_types / num_tokens\n",
    "linguistic_features['TTR'] = num_word_types / num_tokens\n",
    "print('TTR: ', ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to compute TTR, let's look at how we can computer MATTR. \n",
    "\n",
    "In this example, we will compute MATTR with a window size of 20. We start by initializing a few variables. \n",
    "We will compute the TTR for every window of size 20. We need to keep track of the total sum and of the total number of windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "\n",
    "MATTR_sum = 0\n",
    "num_windows = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATTR with window size 20: 0.859449\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = window_size\n",
    "while end < len(pos_tokens):\n",
    "    num_word_types_window = len(set(pos_tokens[start:end]))\n",
    "    num_tokens_window = window_size\n",
    "\n",
    "    MATTR_sum += (num_word_types_window / num_tokens_window)\n",
    "    # Shift window one word at a time\n",
    "    start += 1\n",
    "    end += 1\n",
    "    num_windows += 1\n",
    "\n",
    "mattr = MATTR_sum / num_windows\n",
    "print('MATTR with window size %d: %f' %  (window_size, mattr))\n",
    "linguistic_features['MATTR_%d' % window_size] = mattr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute **Brunet's Index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brunet:  11.048856489696162\n"
     ]
    }
   ],
   "source": [
    "# Brunet\n",
    "brunet = num_tokens ** ( num_word_types**(-0.165))\n",
    "print(\"Brunet: \", brunet)\n",
    "linguistic_features['brunet'] = brunet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we compute the **Honoré statistic**. To do so, we first need to compute the number of words that occur once. We will use the `FreqDist` function in `nltk`, which computes the frequency of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS token:  ('all', 'DT') , frequency:  1\n",
      "POS token:  ('flyers', 'NNS') , frequency:  1\n",
      "POS token:  ('were', 'VBD') , frequency:  1\n",
      "POS token:  ('given', 'VBN') , frequency:  1\n",
      "POS token:  ('a', 'DT') , frequency:  5\n"
     ]
    }
   ],
   "source": [
    "fd_tokens = nltk.probability.FreqDist(pos_tokens)\n",
    "for fd_token, pos_token in zip(list(fd_tokens.values())[:5], pos_tokens[:5]): # Show frequency of first 5 words\n",
    "    print(\"POS token: \", pos_token, \", frequency: \", fd_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of *hapax legomenon* (i.e., words appearing once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words appearing once:  54\n"
     ]
    }
   ],
   "source": [
    "words_appearing_once = 0\n",
    "for num in fd_tokens.values():\n",
    "    if num == 1:\n",
    "        words_appearing_once += 1\n",
    "print(\"Words appearing once: \", words_appearing_once)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honoré:  1397.3211242980462\n"
     ]
    }
   ],
   "source": [
    "honore = 100.0 * math.log(num_tokens)/(1.0 - 1.0 * words_appearing_once / num_word_types)\n",
    "print('Honoré: ', honore)\n",
    "linguistic_features['honore'] = honore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all the features we've extracted so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic features\n",
      "num_tokens :  147\n",
      "average_word_length :  4.210884353741497\n",
      "TTR :  0.5714285714285714\n",
      "MATTR_20 :  0.8594488188976377\n",
      "brunet :  11.048856489696162\n",
      "honore :  1397.3211242980462\n"
     ]
    }
   ],
   "source": [
    "print_linguistic_features(linguistic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Given two vectors *A*, *B*, we can measure the cosine similarity:\n",
    "\\begin{equation}\n",
    "cos(A,B) = \\frac{A \\dot B}{||A|| \\ ||B||}\n",
    "\\end{equation}\n",
    "\n",
    "In order to do this, we first need to convert our words into word vectors. We will start by defining our **vocabulary**, i.e. all unique *non-frequent* words that occur in our transcript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "all flyers were given a purpose and the crow was left without one. he went to visit a bear and the bear spent weeks teaching him about medicine. the crow was still mad without having a purpose. one day he heard a squirrel crying in a hollow oak tree. he went to go see and saw the squirrel was sad about something.  he directed the squirrel to the bear, who was good with medicine. the next the day crow flew around satisfied with knowing that he had helped someone. he heard bunny crying in their burrow. he asked asked why and they said that they could never they could never get any peace with the fox always prying around. then the crow stated the bunny's strengths of big ears and strong legs. then word came around of crow's purpose of helping others find their purpose.\n",
      "\n",
      "Vocabulary:\n",
      "['all', 'burrow', 'without', 'bear', 'cry', 'visit', 'say', 'fox', 'ear', 'could', 'squirrel', 'week', 'prying', 'never', 'purpose', 'then', 'peace', 'teach', 'medicine', \"'s\", 'get', 'oak', 'someone', 'why', 'strong', 'strength', 'one', 'state', 'direct', 'crow', 'go', 'see', 'about', 'around', 'their', 'bunny', 'flyer', 'day', 'have', 'who', 'something', 'help', 'ask', 'always', 'others', 'come', 'any', 'give', 'hollow', 'still', 'word', 'fly', 'that', 'know', 'with', 'leg', 'hear', 'mad', 'next', 'they', 'saw', 'leave', 'find', 'tree', 'he', 'him', 'big', 'sad', 'satisfy', 'spend', 'be', 'good']\n"
     ]
    }
   ],
   "source": [
    "# Define list of words to remove (we ignore words that occur very frequently)\n",
    "words_to_remove = ['the', 'and', 'is', 'a', 'to', 'i', 'on', 'in', 'of', 'it']\n",
    "\n",
    "# Get a list of unique words\n",
    "vocab_words = list(set(lemmatized_tokens))\n",
    "\n",
    "# Remove frequent words\n",
    "for s in words_to_remove:\n",
    "    if s in vocab_words:\n",
    "        vocab_words.remove(s)\n",
    "   \n",
    "print(\"Original text:\")\n",
    "print(text.lower())\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocab_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vocabulary, let's convert each sentence in our transcript into a vector!\n",
    "\n",
    "For every sentence, we will create a vector which has the same size as our vocabulary. At every position, we will assign 1 if the corresponding word occurs, and 0 if it does not.\n",
    "\n",
    "*Example*:\n",
    "\n",
    "**V** =  [\"cat\", \"fat\", \"red\", \"blue\"]\n",
    "\n",
    "- \"The fat cat\" = [1, 1, 0, 0]\n",
    "- \"The red fat cat\" = [1, 1, 1, 0]\n",
    "- \"The blue fat cat\" = [1, 1, 0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['all', 'burrow', 'without', 'bear', 'cry', 'visit', 'say', 'fox', 'ear', 'could', 'squirrel', 'week', 'prying', 'never', 'purpose', 'then', 'peace', 'teach', 'medicine', \"'s\", 'get', 'oak', 'someone', 'why', 'strong', 'strength', 'one', 'state', 'direct', 'crow', 'go', 'see', 'about', 'around', 'their', 'bunny', 'flyer', 'day', 'have', 'who', 'something', 'help', 'ask', 'always', 'others', 'come', 'any', 'give', 'hollow', 'still', 'word', 'fly', 'that', 'know', 'with', 'leg', 'hear', 'mad', 'next', 'they', 'saw', 'leave', 'find', 'tree', 'he', 'him', 'big', 'sad', 'satisfy', 'spend', 'be', 'good']\n",
      "\n",
      "sentence1:  all flyers were given a purpose and the crow was left without one\n",
      "Vector of sentence1:  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    word_vectors.append(len(vocab_words)*[0]) # init\n",
    "    for j in range(len(vocab_words)):\n",
    "        if vocab_words[j] in sentence.lower():\n",
    "            word_vectors[i][j] += 1\n",
    "            \n",
    "# Vector is 1 if the vocabulary word is present in the utterance, 0 otherwise\n",
    "print(\"Vocabulary: \", vocab_words)\n",
    "print(\"\\nsentence1: \", sentences[0].lower())\n",
    "print(\"Vector of sentence1: \", word_vectors[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word vectors, we can compute the cosine distance between each vector pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine distance:  0.7932121704196343\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine distance between each pair of sentences in this transcribe\n",
    "pairs_compared = 0\n",
    "cosine_distance_sum = 0\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i):\n",
    "        # We only compute distance if norms are non-zero\n",
    "        norm_i, norm_j = np.linalg.norm(word_vectors[i]), np.linalg.norm(word_vectors[j])\n",
    "        if norm_i > 0 and norm_j > 0:\n",
    "            pairs_compared += 1\n",
    "            cosine_distance_sum += scipy.spatial.distance.cosine(word_vectors[i], word_vectors[j])\n",
    "\n",
    "average_cosine_distance = cosine_distance_sum / pairs_compared\n",
    "print(\"Average cosine distance: \", average_cosine_distance)\n",
    "linguistic_features['average_cosine_distance'] = cosine_distance_sum / pairs_compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of that just for one feature! ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic features\n",
      "num_tokens :  147\n",
      "average_word_length :  4.210884353741497\n",
      "TTR :  0.5714285714285714\n",
      "MATTR_20 :  0.8594488188976377\n",
      "brunet :  11.048856489696162\n",
      "honore :  1397.3211242980462\n",
      "average_cosine_distance :  0.7932121704196343\n"
     ]
    }
   ],
   "source": [
    "print_linguistic_features(linguistic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valence\n",
    "\n",
    "In this example, we will compute the mean valence of the transcript. We rely on Warriner norms of valence, which provide values of valence for ~13,000 words.\n",
    "- Warriner norms are also used to compute arousal and dominance.\n",
    "- We use the same approach to compute imageability, frequency, age-of-acquisition, subjectivity (except we use norms from other sources).\n",
    "\n",
    "Let's start off by reading in the Warriner norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warriner norms: word: [valence (mean), valence (sd), arousal (mean), arousal (sd), dominance (mean), dominance (sd)]\n",
      "aardvark :  ['6.26', '2.21', '19', '2.41', '1.4', '22', '4.27', '1.75', '15']\n",
      "abalone :  ['5.3', '1.59', '20', '2.65', '1.9', '20', '4.95', '1.79', '22']\n",
      "abandon :  ['2.84', '1.54', '19', '3.73', '2.43', '22', '3.32', '2.5', '22']\n",
      "abandonment :  ['2.63', '1.74', '19', '4.95', '2.64', '21', '2.64', '1.81', '28']\n",
      "abbey :  ['5.85', '1.69', '20', '2.2', '1.7', '20', '5', '2.02', '25']\n"
     ]
    }
   ],
   "source": [
    "# Helper function for reading in dictionary of Warriner norms\n",
    "def get_norms(path_to_norms):\n",
    "    \"\"\"Parameters:\n",
    "    path_to_norms : optional, string. Full path, including filename, of the Warringer norms.\n",
    "\n",
    "    Return dictionary of Warriner norms, order: [warr.V.Mean.Sum,warr.V.SD.Sum,warr.V.Rat.Sum,warr.A.Mean.Sum,warr.A.SD.Sum,warr.A.Rat.Sum,warr.D.Mean.Sum,warr.D.SD.Sum,warr.D.Rat.Sum]\"\"\"\n",
    "\n",
    "    source_norms = path_to_norms\n",
    "    with open(source_norms, \"r\") as fin:\n",
    "        f = fin.readlines()\n",
    "        f = f[1:] # skip header\n",
    "\n",
    "        warr = {}\n",
    "        for line in f:\n",
    "            l = line.strip().split(',')\n",
    "            warr[l[1]] = l[2:11]\n",
    "\n",
    "        return warr\n",
    "\n",
    "warriner_norms = get_norms('libraries/Ratings_Warriner_et_al.csv')\n",
    "print(\"Warriner norms: word: [valence (mean), valence (sd), arousal (mean), arousal (sd), dominance (mean), dominance (sd)]\")\n",
    "for word in list(warriner_norms.keys())[:5]:\n",
    "    print(word, ': ', warriner_norms[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Warriner valence:  5.878783783783784\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean valence of our transcript\n",
    "warriner_valence_sum = 0\n",
    "warriner_valence_words = 0\n",
    "for word in lemmatized_tokens:\n",
    "    if word in warriner_norms:\n",
    "        warriner_valence_sum += float(warriner_norms[word][0])\n",
    "        warriner_valence_words += 1\n",
    "\n",
    "average_warriner_valence = warriner_valence_sum / warriner_valence_words\n",
    "print(\"Average Warriner valence: \", average_warriner_valence)\n",
    "linguistic_features['average_warriner_valence'] = average_warriner_valence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase type length\n",
    "\n",
    "In this example, we will look at the average length of noun phrases.\n",
    "\n",
    "In the feature extraction pipeline, we extract parse trees using the Stanford parser. For this demo, let's assume we already have the Stanford parser output in `data/crow_retelling.txt.parse` :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (S (NP (DT All) (NNS flyers)) (VP (VBD were) (VP (VBN given) (NP (DT a) (NN purpose))))) (CC and) (S (NP (DT the) (NN crow)) (VP (VBD was) (VP (VBN left) (PP (IN without) (NP (CD one)))))) (. .)))\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "ParentedTree('ROOT', [ParentedTree('S', [ParentedTree('S', [ParentedTree('NP', [ParentedTree('DT', ['All']), ParentedTree('NNS', ['flyers'])]), ParentedTree('VP', [ParentedTree('VBD', ['were']), ParentedTree('VP', [ParentedTree('VBN', ['given']), ParentedTree('NP', [ParentedTree('DT', ['a']), ParentedTree('NN', ['purpose'])])])])]), ParentedTree('CC', ['and']), ParentedTree('S', [ParentedTree('NP', [ParentedTree('DT', ['the']), ParentedTree('NN', ['crow'])]), ParentedTree('VP', [ParentedTree('VBD', ['was']), ParentedTree('VP', [ParentedTree('VBN', ['left']), ParentedTree('PP', [ParentedTree('IN', ['without']), ParentedTree('NP', [ParentedTree('CD', ['one'])])])])])]), ParentedTree('.', ['.'])])])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parse tree output\n",
    "with open('data/crow_retelling.txt.parse', 'r') as fin:\n",
    "    treelist = fin.readlines()\n",
    "\n",
    "print(treelist[0])\n",
    "nltk.tree.ParentedTree.fromstring(treelist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through every node in our tree.\n",
    "\n",
    "We compute the number of leaves in any subtree whose root is NP. I print an example subtree and the length of that subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtree\n",
      "(NP (DT All) (NNS flyers))\n",
      "Length of subtree:  2\n",
      "\n",
      "Average length of NP:  2.909090909090909\n"
     ]
    }
   ],
   "source": [
    "length_np_sum = 0\n",
    "np_sum = 0\n",
    "print_first = True\n",
    "for tree in treelist:\n",
    "    t = nltk.tree.Tree.fromstring(tree)\n",
    "    for st in t.subtrees():\n",
    "        if str(st).startswith(\"(NP\"):\n",
    "            if print_first:\n",
    "                print(\"Subtree\")\n",
    "                print(st)\n",
    "                print(\"Length of subtree: \", len(st.leaves()))\n",
    "                print_first = False\n",
    "            length_np_sum += len(st.leaves())\n",
    "            np_sum += 1\n",
    "\n",
    "average_length_np = length_np_sum / np_sum\n",
    "print(\"\")\n",
    "print(\"Average length of NP: \", average_length_np)\n",
    "linguistic_features['average_length_np'] = average_length_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the features we've extracted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic features\n",
      "num_tokens :  147\n",
      "average_word_length :  4.210884353741497\n",
      "TTR :  0.5714285714285714\n",
      "MATTR_20 :  0.8594488188976377\n",
      "brunet :  11.048856489696162\n",
      "honore :  1397.3211242980462\n",
      "average_cosine_distance :  0.7932121704196343\n",
      "average_warriner_valence :  5.878783783783784\n",
      "average_length_np :  2.909090909090909\n"
     ]
    }
   ],
   "source": [
    "print_linguistic_features(linguistic_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
